{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Save2Drive](https://raw.githubusercontent.com/alahnala/AI4All2020-Michigan-NLP/master/slides/save2drive.png)\n",
    "\n",
    "# Language Translation\n",
    "\n",
    "In this project we will be teaching a model to translate from English to Spanish. After you go through this notebook once, you can teach the model to translate from English to French, German, or another language of your choice (just ask us in office hours!) or translate to English from any other language.\n",
    "\n",
    "Before we get started, here is an overview of how language works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"slides/overview.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"slides/overview2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"slides/encoder.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"slides/decoder.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"slides/detail_overview.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from language_translation_help import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data Files\n",
    "\n",
    "The data for this project is a set of many thousands of English to Spanish translation pairs. The file is a tab separated list of translation pairs:\n",
    "\n",
    "```\n",
    "I am cold.    Yo soy frio.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to make the translation easier, we perform several preprocessing steps, including \n",
    "* making all characters lowercase  --> .lower()\n",
    "* stripping white space --> .stri()\n",
    "* trim punctuation --> re.sub(r\"([.!?])\", r\" \\1\", s), re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "\n",
    "\n",
    "After you run through this notebook, you can come back here and play around with this cell. Think about the following questions when you do that:\n",
    "####  What would happen if you didn't lower case all the characters? \n",
    "####  What would happen if you didn't strip the lower case? \n",
    "#### What would happen if you removed things besides punctuation? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_string(s):\n",
    "    s = unicode_to_ascii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering sentences\n",
    "\n",
    "Since there are a *lot* of example sentences and we want to train something relatively quickly, we'll trim the data set to only relatively short and simple sentences. We're filtering to sentences that translate to the form \"I am\" or \"He is\" etc. (accounting for apostrophes being removed). \n",
    "\n",
    "After you go through this notebook, feel free to change these prefixes or add to them and see how that affects your model. You can look through the data files in the data folder and see which prefixes are used that are not included here for ideas on what to add in this section. Think about the following question when you do this:\n",
    "\n",
    "#### Why do you think we include contractions? (ex. \"i am\" as well as \"i m\"). Do you see a decrease or increase in the performance of the encoder and decoder when removing contractions?\n",
    "#### What are some other prefixes you chose to add/ remove here? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "good_prefixes = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s\",\n",
    "    \"you are\", \"you re \"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have some functions to help us filter our data into sentences that have \"good prefixes.\" If you decide that you want to perform a translation from a language to English, you can change the variable english_to in the function below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_pair(p, good_prefixes):\n",
    "    # change the following variable from True to False if you want to translate a certain language TO English.\n",
    "    # This variable being True indicates that we are translating English into another language\n",
    "    english_to = True\n",
    "    if english_to == True:\n",
    "        return len(p[0].split(' ')) < MAX_LENGTH and len(p[1].split(' ')) < MAX_LENGTH and \\\n",
    "            p[0].startswith(good_prefixes)\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and len(p[1].split(' ')) < MAX_LENGTH and \\\n",
    "            p[1].startswith(good_prefixes)\n",
    "\n",
    "def prepare_data(lang1_name, lang2_name, reverse=False):\n",
    "    input_lang, output_lang, pairs = read_langs(lang1_name, lang2_name, normalize_string, reverse)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    pairs = filter_pairs(pairs, good_prefixes, filter_pair)\n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    print(\"Indexing words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.index_words(pair[0])\n",
    "        output_lang.index_words(pair[1])\n",
    "\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we prepare our final data to input into our encoder and decoder using the \"prepare data\" function. It takes in 3 variables:\n",
    "* lang1: a language we want to translate from or to. This always going to be 'eng', which is short for 'English.'\n",
    "* lang2: a language we want to translate from or to. This is set by default to 'spa', which is short for 'Spanish.'\n",
    "* reverse: False if we want to translate from lang1 to lang2, True if we want to translate from lang2 to lang1\n",
    "\n",
    "If you want to translate **from** Spanish **to** English, set:\n",
    "* lang1 = 'eng'\n",
    "* lang2 = 'spa'\n",
    "* reverse: True\n",
    "* english_to in the filter_pair function (above) to False\n",
    "\n",
    "If you want to translate **from** English **to** French, set:\n",
    "* lang1 = 'eng'\n",
    "* lang2 = 'fra'\n",
    "* reverse: False\n",
    "* english_to in the filter_pair function (above) to True\n",
    "\n",
    "If you want to translate **from** French **to** English\n",
    "* lang1 = 'eng'\n",
    "* lang2 = 'fra'\n",
    "* reverse: True\n",
    "* english_to in the filter_pair function to False\n",
    "\n",
    "If you want to translate **from** English **to** German\n",
    "* lang1 = 'eng'\n",
    "* lang2 = 'deu'\n",
    "* reverse: False\n",
    "* english_to in the filter_pair function (above) to True\n",
    "\n",
    "If you want to translate **from** German **to** English\n",
    "* lang1 = 'eng'\n",
    "* lang2 = 'deu'\n",
    "* reverse: True\n",
    "* english_to in the filter_pair function to False\n",
    "* english_to in the filter_pair function (above) to False\n",
    "\n",
    "#### This function outputs pairs of phrases in \"initial_lang\" and \"final_lang\", AKA the languages you want to translate from and to. We print an example pair at the end of the cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 124325 sentence pairs\n",
      "Trimmed to 6840 sentence pairs\n",
      "Indexing words...\n",
      "['he is always right .', 'el siempre tiene razon .']\n"
     ]
    }
   ],
   "source": [
    "initial_lang = 'eng'\n",
    "final_lang = 'spa'\n",
    "reverse = False\n",
    "\n",
    "input_lang, output_lang, pairs = prepare_data(initial_lang, final_lang, reverse)\n",
    "\n",
    "# Print an example pair\n",
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Testing the Encoder and Decoder\n",
    "The exact inputs and outputs are not exactly important for this cell. I just wanted you to get a little bit of intuition on how the encoders and decoders work. We start with a certain input, \"word_input\", initialize an encoder, \"encoder_test\", and run the encoder using both of those. We take the output of the encoder, \"all_encoder_outputs\", and put that into the decoder, along with the initialized decoder, \"decoder_test\", and the initial input to produce our final outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data3/home/meerak/LanguageTranslation/language_translation_help.py:189: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.softmax(attn_energies).unsqueeze(0).unsqueeze(0)\n",
      "/data3/home/meerak/LanguageTranslation/language_translation_help.py:253: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  output = F.log_softmax(self.out(torch.cat((rnn_output, context), 1)))\n"
     ]
    }
   ],
   "source": [
    "word_input = Variable(torch.LongTensor([1, 2, 3]))\n",
    "encoder_test = create_encoder()\n",
    "decoder_test =  create_decoder()\n",
    "\n",
    "all_encoder_outputs = run_encoder(encoder_test, word_input)\n",
    "_ = run_decoder(decoder_test, word_input, all_encoder_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Our Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"slides/training.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step to \"training\" is initializing our encoder and decoder. We do this in one step, and have it hidden in a helper function for ease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "all_vars_training = init_vars(input_lang, output_lang)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, **n_epochs** is the amount of time that we want to train for. A unit of time in this case is an \"epoch.\" After going through this file, you can play around with this number. Think about the following questions:\n",
    "#### Would increasing or decreasing n_epochs improve performance? Why?\n",
    "#### Do you notice a big difference in the translation ability of your encoder/decoder when you increase/decrease n_epochs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Configuring training\n",
    "n_epochs = 10000\n",
    "plot_every = 200\n",
    "print_every = 100\n",
    "\n",
    "# Keep track of time elapsed and running averages\n",
    "start = time.time()\n",
    "plot_losses = []\n",
    "print_loss_total = 0 # Reset every print_every\n",
    "plot_loss_total = 0 # Reset every plot_every"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, we train our encoder and decoder! At each step, we compute a value called \"loss\", which is an indication of how bad our model is at language translation at the time (the higher the loss, the worse our model is at language translation). The loss should decrease over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data3/home/meerak/LanguageTranslation/language_translation_help.py:352: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  torch.nn.utils.clip_grad_norm(encoder.parameters(), clip)\n",
      "/data3/home/meerak/LanguageTranslation/language_translation_help.py:353: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  torch.nn.utils.clip_grad_norm(decoder.parameters(), clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0m 4s (- 6m 44s) (100 1%) 4.8663\n",
      "0m 8s (- 6m 50s) (200 2%) 4.3001\n",
      "0m 12s (- 6m 47s) (300 3%) 4.0198\n",
      "0m 16s (- 6m 44s) (400 4%) 4.2121\n",
      "0m 20s (- 6m 33s) (500 5%) 4.0260\n",
      "0m 24s (- 6m 29s) (600 6%) 3.9439\n",
      "0m 28s (- 6m 23s) (700 7%) 4.0747\n",
      "0m 33s (- 6m 29s) (800 8%) 4.1131\n",
      "0m 38s (- 6m 33s) (900 9%) 3.8772\n",
      "0m 43s (- 6m 27s) (1000 10%) 3.8002\n",
      "0m 47s (- 6m 24s) (1100 11%) 3.7382\n",
      "0m 52s (- 6m 21s) (1200 12%) 3.7438\n",
      "0m 56s (- 6m 19s) (1300 13%) 3.9049\n",
      "1m 1s (- 6m 18s) (1400 14%) 3.9732\n",
      "1m 6s (- 6m 16s) (1500 15%) 3.7495\n",
      "1m 11s (- 6m 13s) (1600 16%) 3.8046\n",
      "1m 16s (- 6m 12s) (1700 17%) 3.6501\n",
      "1m 21s (- 6m 11s) (1800 18%) 3.6601\n",
      "1m 27s (- 6m 12s) (1900 19%) 3.7182\n",
      "1m 32s (- 6m 9s) (2000 20%) 3.5589\n",
      "1m 37s (- 6m 5s) (2100 21%) 3.6324\n",
      "1m 42s (- 6m 2s) (2200 22%) 3.5575\n",
      "1m 47s (- 6m 1s) (2300 23%) 3.5826\n",
      "1m 53s (- 5m 59s) (2400 24%) 3.6394\n",
      "1m 58s (- 5m 55s) (2500 25%) 3.5538\n",
      "2m 3s (- 5m 51s) (2600 26%) 3.5793\n",
      "2m 8s (- 5m 47s) (2700 27%) 3.5440\n",
      "2m 13s (- 5m 44s) (2800 28%) 3.3565\n",
      "2m 19s (- 5m 40s) (2900 28%) 3.5622\n",
      "2m 24s (- 5m 37s) (3000 30%) 3.2798\n",
      "2m 29s (- 5m 33s) (3100 31%) 3.5463\n",
      "2m 34s (- 5m 28s) (3200 32%) 3.3969\n",
      "2m 39s (- 5m 24s) (3300 33%) 3.2220\n",
      "2m 45s (- 5m 20s) (3400 34%) 3.3353\n",
      "2m 50s (- 5m 15s) (3500 35%) 3.3610\n",
      "2m 55s (- 5m 11s) (3600 36%) 3.4326\n",
      "3m 0s (- 5m 7s) (3700 37%) 3.2870\n",
      "3m 4s (- 5m 1s) (3800 38%) 3.1736\n",
      "3m 10s (- 4m 57s) (3900 39%) 3.2684\n",
      "3m 15s (- 4m 53s) (4000 40%) 3.3391\n",
      "3m 21s (- 4m 49s) (4100 41%) 3.3436\n",
      "3m 26s (- 4m 44s) (4200 42%) 3.0645\n",
      "3m 31s (- 4m 40s) (4300 43%) 3.1399\n",
      "3m 36s (- 4m 36s) (4400 44%) 3.0492\n",
      "3m 42s (- 4m 31s) (4500 45%) 3.3132\n",
      "3m 47s (- 4m 26s) (4600 46%) 3.1240\n",
      "3m 51s (- 4m 21s) (4700 47%) 3.2262\n",
      "3m 56s (- 4m 16s) (4800 48%) 3.1894\n",
      "4m 1s (- 4m 11s) (4900 49%) 2.9765\n",
      "4m 6s (- 4m 6s) (5000 50%) 2.9026\n",
      "4m 11s (- 4m 1s) (5100 51%) 2.8308\n",
      "4m 16s (- 3m 56s) (5200 52%) 3.2924\n",
      "4m 21s (- 3m 51s) (5300 53%) 2.9073\n"
     ]
    }
   ],
   "source": [
    "# Begin!\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    # Get phrase in language to translate from (input variable, default = English phrase) and\n",
    "    # phrase in language to translate to (target variable, default = Spanish phrase)\n",
    "    training_pair = variables_from_pair(random.choice(pairs), input_lang, output_lang)\n",
    "    input_variable = training_pair[0]\n",
    "    target_variable = training_pair[1]\n",
    "\n",
    "    # Run the train function\n",
    "    loss = train(input_variable, target_variable, all_vars_training)\n",
    "\n",
    "    # Keep track of loss\n",
    "    print_loss_total += loss\n",
    "    plot_loss_total += loss\n",
    "\n",
    "    if epoch == 0: continue\n",
    "\n",
    "    if epoch % print_every == 0:\n",
    "        print_loss_avg = print_loss_total / print_every\n",
    "        print_loss_total = 0\n",
    "        print_summary = '%s (%d %d%%) %.4f' % (time_since(start, epoch / n_epochs), epoch, epoch / n_epochs * 100, print_loss_avg)\n",
    "        print(print_summary)\n",
    "        \n",
    "    if epoch % plot_every == 0:\n",
    "        plot_loss_avg = plot_loss_total / plot_every\n",
    "        plot_losses.append(plot_loss_avg)\n",
    "        plot_loss_total = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, you can see the loss decreasing over time, as our encoder and decoder get better at language translation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "def show_plot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    loc = ticker.MultipleLocator(base=0.2) # put ticks at regular intervals\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)\n",
    "\n",
    "show_plot(plot_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have trained our encoder and decoder, we can use them to perform translations! Below, in the \"evaluate_randomly\" function, we randomly pick a pair of phrases that we have trained on, and see how well we can translate that phrase. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for_evaluations = (input_lang, output_lang, all_vars_training[0], all_vars_training[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_randomly():\n",
    "    pair = random.choice(pairs)\n",
    "    output_words, decoder_attn = evaluate(pair[0], for_evaluations)\n",
    "    output_sentence = ' '.join(output_words)\n",
    "    \n",
    "    print('>', pair[0])\n",
    "    print('=', pair[1])\n",
    "    print('<', output_sentence)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can keep running this cell over and over again to see how well the translator does on various phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "evaluate_randomly()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also evaluate the encoder/decoder on phrases that you come up with! Here is an example of how to do that. \n",
    "### Note\n",
    "If you translated from English to another language, the phrases you test have to start with the \"good prefixes\" and also contain words that the model has seen before. This is why you may get errors if you change the \"phrase\" below.\n",
    "\n",
    "If you translated from another language to English, you have to select phrases that start with translated \"good prefixes\" and also contain translated words the model has seen before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase = 'i m happy .'\n",
    "output_words, _ = evaluate(phrase, for_evaluations)\n",
    "output_sentence = ' '.join(output_words)\n",
    "print('>', phrase)\n",
    "print('<', output_sentence)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
