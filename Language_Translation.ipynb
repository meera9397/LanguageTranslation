{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Save2Drive](https://raw.githubusercontent.com/alahnala/AI4All2020-Michigan-NLP/master/slides/save2drive.png)\n",
    "\n",
    "# Language Translation\n",
    "\n",
    "In this project we will be teaching a model to translate from English to Spanish. After you go through this notebook once, you can teach the model to translate from English to French, German, or another language of your choice (just ask us in office hours!) or translate to English from any other language.\n",
    "\n",
    "Before we get started, here is an overview of how language translation works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Save2Drive](https://raw.githubusercontent.com/meera9397/LanguageTranslation/master/slides/overview.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Save2Drive](https://raw.githubusercontent.com/meera9397/LanguageTranslation/master/slides/overview2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Save2Drive](https://raw.githubusercontent.com/meera9397/LanguageTranslation/master/slides/encoder.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Save2Drive](https://raw.githubusercontent.com/meera9397/LanguageTranslation/master/slides/decoder.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Save2Drive](https://raw.githubusercontent.com/meera9397/LanguageTranslation/master/slides/detail_overview.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to run this notebook, you have to do the following steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Save2Drive](https://raw.githubusercontent.com/meera9397/LanguageTranslation/master/slides/run1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Save2Drive](https://raw.githubusercontent.com/meera9397/LanguageTranslation/master/slides/run2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    !rm -r LanguageTranslation\n",
    "    !git clone https://github.com/meera9397/LanguageTranslation.git\n",
    "    !cp -r LanguageTranslation/data/ .\n",
    "    !cp -r LanguageTranslation/slides/ .\n",
    "    !cp -r LanguageTranslation/utils/ .\n",
    "    !echo \"=== Files Copied ===\"\n",
    "    \n",
    "from utils.language_translation_help import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data Files\n",
    "\n",
    "The data for this project is a set of many thousands of English to Spanish translation pairs. The file is a tab separated list of translation pairs:\n",
    "\n",
    "```\n",
    "I am cold.    Yo soy frio.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the cell below to load the `normalize_string` function\n",
    "In order to make the translation easier, we perform several preprocessing steps, including \n",
    "* making all characters lowercase  --> .lower()\n",
    "* stripping white space --> .stri()\n",
    "* trim punctuation --> re.sub(r\"([.!?])\", r\" \\1\", s), re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "\n",
    "\n",
    "After you run through this notebook, you can come back here and play around with this cell. Think about the following questions when you do that:\n",
    "####  What would happen if you didn't lower case all the characters? \n",
    "####  What would happen if you didn't strip the lower case? \n",
    "#### What would happen if you removed things besides punctuation? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function loaded\n"
     ]
    }
   ],
   "source": [
    "def normalize_string(s):\n",
    "    s = unicode_to_ascii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s\n",
    "print(\"Function loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering sentences\n",
    "\n",
    "Since there are a *lot* of example sentences and we want to train something relatively quickly, we'll trim the data set to only relatively short and simple sentences. We're filtering to sentences that translate to the form \"I am\" or \"He is\" etc. (accounting for apostrophes being removed). \n",
    "\n",
    "After you go through this notebook, feel free to change these prefixes or add to them and see how that affects your model. You can look through the data files in the data folder and see which prefixes are used that are not included here for ideas on what to add in this section. Think about the following questions when you do this:\n",
    "\n",
    "#### Why do you think we include contractions? (ex. \"i am\" as well as \"i m\"). Do you see a decrease or increase in the performance of the encoder and decoder when removing contractions?\n",
    "#### What are some other prefixes you chose to add/ remove here? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good_prefixes loaded\n"
     ]
    }
   ],
   "source": [
    "# run to load good_prefixes\n",
    "good_prefixes = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s\",\n",
    "    \"you are\", \"you re \"\n",
    ")\n",
    "\n",
    "print(\"good_prefixes loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have some functions to help us filter our data into sentences that have \"good prefixes.\" If you decide that you want to perform a translation from a language to English, you can change the variable english_to in the function below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Functions loaded\n"
     ]
    }
   ],
   "source": [
    "# run to load functions\n",
    "def filter_pair(p, good_prefixes):\n",
    "    # change the following variable from True to False if you want to translate a certain language TO English.\n",
    "    # This variable being True indicates that we are translating English into another language\n",
    "    english_to = True\n",
    "    if english_to == True:\n",
    "        return len(p[0].split(' ')) < MAX_LENGTH and len(p[1].split(' ')) < MAX_LENGTH and \\\n",
    "            p[0].startswith(good_prefixes)\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and len(p[1].split(' ')) < MAX_LENGTH and \\\n",
    "            p[1].startswith(good_prefixes)\n",
    "\n",
    "def prepare_data(lang1_name, lang2_name, reverse=False):\n",
    "    input_lang, output_lang, pairs = read_langs(lang1_name, lang2_name, normalize_string, reverse)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    pairs = filter_pairs(pairs, good_prefixes, filter_pair)\n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    print(\"Indexing words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.index_words(pair[0])\n",
    "        output_lang.index_words(pair[1])\n",
    "\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "print(\"Functions loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we prepare our final data to input into our encoder and decoder using the \"prepare data\" function. It takes in 3 variables:\n",
    "* lang1: a language we want to translate from or to. This always going to be 'eng', which is short for 'English.'\n",
    "* lang2: a language we want to translate from or to. This is set by default to 'spa', which is short for 'Spanish.'\n",
    "* reverse: False if we want to translate from lang1 to lang2, True if we want to translate from lang2 to lang1\n",
    "\n",
    "If you want to translate **from** Spanish **to** English, set:\n",
    "* lang1 = 'eng'\n",
    "* lang2 = 'spa'\n",
    "* reverse: True\n",
    "* english_to in the filter_pair function (above) to False\n",
    "\n",
    "If you want to translate **from** English **to** French, set:\n",
    "* lang1 = 'eng'\n",
    "* lang2 = 'fra'\n",
    "* reverse: False\n",
    "* english_to in the filter_pair function (above) to True\n",
    "\n",
    "If you want to translate **from** French **to** English\n",
    "* lang1 = 'eng'\n",
    "* lang2 = 'fra'\n",
    "* reverse: True\n",
    "* english_to in the filter_pair function to False\n",
    "\n",
    "If you want to translate **from** English **to** German\n",
    "* lang1 = 'eng'\n",
    "* lang2 = 'deu'\n",
    "* reverse: False\n",
    "* english_to in the filter_pair function (above) to True\n",
    "\n",
    "If you want to translate **from** German **to** English\n",
    "* lang1 = 'eng'\n",
    "* lang2 = 'deu'\n",
    "* reverse: True\n",
    "* english_to in the filter_pair function (above) to False\n",
    "\n",
    "#### This function outputs pairs of phrases in \"lang1\" and \"lang2\", AKA the languages you want to translate from and to. We print an example pair at the end of the cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 124325 sentence pairs\n",
      "Trimmed to 6840 sentence pairs\n",
      "Indexing words...\n",
      "['you re lucky you didn t get shot .', 'tienes suerte de que no te dispararan .']\n"
     ]
    }
   ],
   "source": [
    "# run cell\n",
    "\n",
    "lang1 = 'eng'\n",
    "lang2 = 'spa'\n",
    "reverse = False\n",
    "\n",
    "input_lang, output_lang, pairs = prepare_data(lang1, lang2, reverse)\n",
    "\n",
    "# Print an example pair\n",
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Testing the Encoder and Decoder\n",
    "The exact inputs and outputs are not exactly important for this cell. I just wanted you to get a little bit of intuition on how the encoders and decoders work. We start with a certain input, \"word_input\", initialize an encoder, \"encoder_test\", and run the encoder using both of those. We take the output of the encoder, \"all_encoder_outputs\", and put that into the decoder, along with the initialized decoder, \"decoder_test\", and the initial input to produce our final outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data3/home/meerak/LanguageTranslation/utils/language_translation_help.py:189: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.softmax(attn_energies).unsqueeze(0).unsqueeze(0)\n",
      "/data3/home/meerak/LanguageTranslation/utils/language_translation_help.py:253: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  output = F.log_softmax(self.out(torch.cat((rnn_output, context), 1)))\n"
     ]
    }
   ],
   "source": [
    "# run cell\n",
    "\n",
    "word_input = Variable(torch.LongTensor([1, 2, 3]))\n",
    "encoder_test = create_encoder()\n",
    "decoder_test =  create_decoder()\n",
    "\n",
    "all_encoder_outputs = run_encoder(encoder_test, word_input)\n",
    "_ = run_decoder(decoder_test, word_input, all_encoder_outputs)\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Our Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Save2Drive](https://raw.githubusercontent.com/meera9397/LanguageTranslation/master/slides/training.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step to \"training\" is initializing our encoder and decoder. We do this in one step, and have it hidden in a helper function for ease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Run cell to initialize models\n",
    "all_vars_training = init_vars(input_lang, output_lang)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, **n_epochs** is the amount of time that we want to train for. A unit of time in this case is an \"epoch.\" After going through this file, you can play around with this number. Think about the following questions:\n",
    "#### Would increasing or decreasing n_epochs improve performance? Why?\n",
    "#### Do you notice a big difference in the translation ability of your encoder/decoder when you increase/decrease n_epochs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Run cell to configuring training\n",
    "n_epochs = 10000\n",
    "plot_every = 200\n",
    "print_every = 100\n",
    "\n",
    "# Keep track of time elapsed and running averages\n",
    "start = time.time()\n",
    "plot_losses = []\n",
    "print_loss_total = 0 # Reset every print_every\n",
    "plot_loss_total = 0 # Reset every plot_every"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, we train our encoder and decoder! At each step, we compute a value called \"loss\", which is an indication of how bad our model is at language translation at the time (the higher the loss, the worse our model is at language translation). The loss should decrease over time.\n",
    "\n",
    "**This will take a few minutes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data3/home/meerak/LanguageTranslation/utils/language_translation_help.py:352: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  torch.nn.utils.clip_grad_norm(encoder.parameters(), clip)\n",
      "/data3/home/meerak/LanguageTranslation/utils/language_translation_help.py:353: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  torch.nn.utils.clip_grad_norm(decoder.parameters(), clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100: 0m 4s (- 7m 24s) (100 1%) 5.2308\n",
      "Epoch 200: 0m 8s (- 7m 11s) (200 2%) 4.1903\n",
      "Epoch 300: 0m 13s (- 7m 11s) (300 3%) 4.0627\n",
      "Epoch 400: 0m 18s (- 7m 16s) (400 4%) 4.0386\n",
      "Epoch 500: 0m 22s (- 7m 4s) (500 5%) 4.0142\n",
      "Epoch 600: 0m 26s (- 6m 59s) (600 6%) 4.0762\n",
      "Epoch 700: 0m 31s (- 7m 2s) (700 7%) 4.0942\n",
      "Epoch 800: 0m 36s (- 7m 4s) (800 8%) 4.0311\n",
      "Epoch 900: 0m 41s (- 7m 1s) (900 9%) 3.9425\n",
      "Epoch 1000: 0m 46s (- 7m 1s) (1000 10%) 3.9408\n",
      "Epoch 1100: 0m 51s (- 6m 57s) (1100 11%) 4.1068\n",
      "Epoch 1200: 0m 56s (- 6m 52s) (1200 12%) 3.8151\n",
      "Epoch 1300: 1m 0s (- 6m 45s) (1300 13%) 3.8863\n",
      "Epoch 1400: 1m 5s (- 6m 42s) (1400 14%) 3.7786\n",
      "Epoch 1500: 1m 9s (- 6m 35s) (1500 15%) 3.6512\n",
      "Epoch 1600: 1m 14s (- 6m 31s) (1600 16%) 3.7794\n",
      "Epoch 1700: 1m 19s (- 6m 28s) (1700 17%) 3.6807\n",
      "Epoch 1800: 1m 24s (- 6m 24s) (1800 18%) 3.7887\n",
      "Epoch 1900: 1m 29s (- 6m 19s) (1900 19%) 3.7335\n",
      "Epoch 2000: 1m 33s (- 6m 15s) (2000 20%) 3.6269\n",
      "Epoch 2100: 1m 38s (- 6m 10s) (2100 21%) 3.6734\n",
      "Epoch 2200: 1m 43s (- 6m 5s) (2200 22%) 3.6281\n",
      "Epoch 2300: 1m 47s (- 6m 1s) (2300 23%) 3.6094\n",
      "Epoch 2400: 1m 53s (- 5m 58s) (2400 24%) 3.5660\n",
      "Epoch 2500: 1m 57s (- 5m 53s) (2500 25%) 3.4660\n",
      "Epoch 2600: 2m 2s (- 5m 49s) (2600 26%) 3.6018\n",
      "Epoch 2700: 2m 8s (- 5m 46s) (2700 27%) 3.6255\n",
      "Epoch 2800: 2m 13s (- 5m 42s) (2800 28%) 3.6655\n",
      "Epoch 2900: 2m 17s (- 5m 37s) (2900 28%) 3.4414\n",
      "Epoch 3000: 2m 23s (- 5m 34s) (3000 30%) 3.5340\n",
      "Epoch 3100: 2m 27s (- 5m 29s) (3100 31%) 3.4526\n",
      "Epoch 3200: 2m 32s (- 5m 24s) (3200 32%) 3.5086\n",
      "Epoch 3300: 2m 37s (- 5m 20s) (3300 33%) 3.6270\n",
      "Epoch 3400: 2m 42s (- 5m 15s) (3400 34%) 3.4727\n",
      "Epoch 3500: 2m 47s (- 5m 10s) (3500 35%) 3.4901\n",
      "Epoch 3600: 2m 52s (- 5m 6s) (3600 36%) 3.1936\n",
      "Epoch 3700: 2m 56s (- 5m 0s) (3700 37%) 3.3865\n",
      "Epoch 3800: 3m 1s (- 4m 56s) (3800 38%) 3.2172\n",
      "Epoch 3900: 3m 6s (- 4m 52s) (3900 39%) 3.6209\n",
      "Epoch 4000: 3m 11s (- 4m 47s) (4000 40%) 3.3143\n",
      "Epoch 4100: 3m 16s (- 4m 42s) (4100 41%) 3.1365\n",
      "Epoch 4200: 3m 21s (- 4m 38s) (4200 42%) 3.3155\n",
      "Epoch 4300: 3m 26s (- 4m 33s) (4300 43%) 3.2414\n",
      "Epoch 4400: 3m 32s (- 4m 29s) (4400 44%) 3.3793\n",
      "Epoch 4500: 3m 36s (- 4m 25s) (4500 45%) 3.1243\n",
      "Epoch 4600: 3m 42s (- 4m 20s) (4600 46%) 3.1046\n",
      "Epoch 4700: 3m 47s (- 4m 15s) (4700 47%) 3.1276\n",
      "Epoch 4800: 3m 51s (- 4m 11s) (4800 48%) 3.2461\n",
      "Epoch 4900: 3m 56s (- 4m 6s) (4900 49%) 3.2138\n",
      "Epoch 5000: 4m 1s (- 4m 1s) (5000 50%) 3.0351\n",
      "Epoch 5100: 4m 6s (- 3m 56s) (5100 51%) 3.0975\n",
      "Epoch 5200: 4m 11s (- 3m 52s) (5200 52%) 3.0348\n",
      "Epoch 5300: 4m 15s (- 3m 47s) (5300 53%) 3.1512\n",
      "Epoch 5400: 4m 21s (- 3m 42s) (5400 54%) 3.0888\n",
      "Epoch 5500: 4m 25s (- 3m 37s) (5500 55%) 3.2415\n",
      "Epoch 5600: 4m 30s (- 3m 32s) (5600 56%) 2.9723\n",
      "Epoch 5700: 4m 35s (- 3m 27s) (5700 56%) 2.9329\n",
      "Epoch 5800: 4m 40s (- 3m 23s) (5800 57%) 3.0276\n",
      "Epoch 5900: 4m 45s (- 3m 18s) (5900 59%) 3.2168\n",
      "Epoch 6000: 4m 50s (- 3m 13s) (6000 60%) 2.8956\n",
      "Epoch 6100: 4m 55s (- 3m 8s) (6100 61%) 2.8661\n",
      "Epoch 6200: 5m 0s (- 3m 3s) (6200 62%) 3.0864\n",
      "Epoch 6300: 5m 4s (- 2m 58s) (6300 63%) 2.8496\n",
      "Epoch 6400: 5m 9s (- 2m 54s) (6400 64%) 2.9812\n",
      "Epoch 6500: 5m 14s (- 2m 49s) (6500 65%) 2.8485\n",
      "Epoch 6600: 5m 18s (- 2m 44s) (6600 66%) 2.8170\n",
      "Epoch 6700: 5m 23s (- 2m 39s) (6700 67%) 3.1057\n",
      "Epoch 6800: 5m 28s (- 2m 34s) (6800 68%) 2.9647\n",
      "Epoch 6900: 5m 33s (- 2m 29s) (6900 69%) 2.7892\n",
      "Epoch 7000: 5m 39s (- 2m 25s) (7000 70%) 2.7655\n",
      "Epoch 7100: 5m 44s (- 2m 20s) (7100 71%) 3.0774\n",
      "Epoch 7200: 5m 49s (- 2m 15s) (7200 72%) 2.7789\n",
      "Epoch 7300: 5m 54s (- 2m 11s) (7300 73%) 2.9676\n",
      "Epoch 7400: 5m 59s (- 2m 6s) (7400 74%) 2.8142\n",
      "Epoch 7500: 6m 4s (- 2m 1s) (7500 75%) 2.8453\n",
      "Epoch 7600: 6m 9s (- 1m 56s) (7600 76%) 2.6372\n",
      "Epoch 7700: 6m 14s (- 1m 51s) (7700 77%) 2.9676\n",
      "Epoch 7800: 6m 19s (- 1m 46s) (7800 78%) 2.6438\n",
      "Epoch 7900: 6m 23s (- 1m 42s) (7900 79%) 2.9573\n",
      "Epoch 8000: 6m 28s (- 1m 37s) (8000 80%) 2.7160\n",
      "Epoch 8100: 6m 33s (- 1m 32s) (8100 81%) 2.8522\n",
      "Epoch 8200: 6m 38s (- 1m 27s) (8200 82%) 2.7726\n",
      "Epoch 8300: 6m 43s (- 1m 22s) (8300 83%) 2.6903\n",
      "Epoch 8400: 6m 49s (- 1m 17s) (8400 84%) 2.6498\n",
      "Epoch 8500: 6m 53s (- 1m 13s) (8500 85%) 2.7472\n",
      "Epoch 8600: 6m 58s (- 1m 8s) (8600 86%) 2.7883\n"
     ]
    }
   ],
   "source": [
    "# Run cell to start training!\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    # Get phrase in language to translate from (input variable, default = English phrase) and\n",
    "    # phrase in language to translate to (target variable, default = Spanish phrase)\n",
    "    training_pair = variables_from_pair(random.choice(pairs), input_lang, output_lang)\n",
    "    input_variable = training_pair[0]\n",
    "    target_variable = training_pair[1]\n",
    "\n",
    "    # Run the train function\n",
    "    loss = train(input_variable, target_variable, all_vars_training)\n",
    "\n",
    "    # Keep track of loss\n",
    "    print_loss_total += loss\n",
    "    plot_loss_total += loss\n",
    "\n",
    "    if epoch == 0: continue\n",
    "\n",
    "    if epoch % print_every == 0:\n",
    "        print_loss_avg = print_loss_total / print_every\n",
    "        print_loss_total = 0\n",
    "        print_summary = 'Epoch %s: %s (%d %d%%) %.4f' % (epoch, time_since(start, epoch / n_epochs), epoch, epoch / n_epochs * 100, print_loss_avg)\n",
    "        print(print_summary)\n",
    "        \n",
    "    if epoch % plot_every == 0:\n",
    "        plot_loss_avg = plot_loss_total / plot_every\n",
    "        plot_losses.append(plot_loss_avg)\n",
    "        plot_loss_total = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, you can see the loss decreasing over time, as our encoder and decoder get better at language translation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "def show_plot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    loc = ticker.MultipleLocator(base=0.2) # put ticks at regular intervals\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)\n",
    "\n",
    "show_plot(plot_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have trained our encoder and decoder, we can use them to perform translations! Below, in the \"evaluate_randomly\" function, we randomly pick a pair of phrases that we have trained on, and see how well we can translate that phrase. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for_evaluations = (input_lang, output_lang, all_vars_training[0], all_vars_training[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_randomly():\n",
    "    pair = random.choice(pairs)\n",
    "    output_words, decoder_attn = evaluate(pair[0], for_evaluations)\n",
    "    output_sentence = ' '.join(output_words)\n",
    "    \n",
    "    print('>', pair[0])\n",
    "    print('=', pair[1])\n",
    "    print('<', output_sentence)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can keep running this cell over and over again to see how well the translator does on various phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "evaluate_randomly()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also evaluate the encoder/decoder on phrases that you come up with! Here is an example of how to do that. \n",
    "### Note\n",
    "If you translated from English to another language, the phrases you test have to start with the \"good prefixes\" and also contain words that the model has seen before. This is why you may get errors if you change the \"phrase\" below.\n",
    "\n",
    "If you translated from another language to English, you have to select phrases that start with translated \"good prefixes\" and also contain translated words the model has seen before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase = 'i m happy .'\n",
    "output_words, _ = evaluate(phrase, for_evaluations)\n",
    "output_sentence = ' '.join(output_words)\n",
    "print('>', phrase)\n",
    "print('<', output_sentence)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
